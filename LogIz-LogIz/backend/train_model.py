import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
import pickle
import io

def load_and_clean_csv(filename):
    """CSV dosyasÄ±nÄ± okur ve tÄ±rnak iÅŸaretlerini temizler"""
    with open(filename, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # TÄ±rnaklarÄ± temizle (EÄŸer tÃ¼m satÄ±r "..." iÃ§indeyse)
    lines = content.splitlines()
    cleaned_lines = []
    for line in lines:
        line = line.strip()
        if line.startswith('"') and line.endswith('"'):
            line = line[1:-1]
        cleaned_lines.append(line)
    
    cleaned_content = "\n".join(cleaned_lines)
    return pd.read_csv(io.StringIO(cleaned_content))

print("ğŸ“Š Veriler yÃ¼kleniyor...")
# 1. Mevcut sample veriyi yÃ¼kle (SaldÄ±rÄ± datasÄ± iÃ§in)
df_sample = load_and_clean_csv('unsw_sample.csv')
print(f"Sample veri: {len(df_sample)} satÄ±r")

# 2. KullanÄ±cÄ± verisini yÃ¼kle (Normal data)
try:
    df_user = load_and_clean_csv('data.csv')
    print(f"User veri: {len(df_user)} satÄ±r")
except Exception as e:
    print(f"âš ï¸ User veri yÃ¼klenemedi: {e}")
    df_user = pd.DataFrame()

# 3. 123.csv dosyasÄ±nÄ± yÃ¼kle (BÃ¼yÃ¼k veri)
try:
    df_large = load_and_clean_csv('123.csv')
    print(f"Large veri (123.csv): {len(df_large)} satÄ±r")
except Exception as e:
    print(f"âš ï¸ Large veri yÃ¼klenemedi: {e}")
    df_large = pd.DataFrame()

# 4. Verileri birleÅŸtir
datasets = [df_sample, df_user, df_large]
valid_datasets = [d for d in datasets if not d.empty]

if valid_datasets:
    # TÃ¼m datasetlerdeki ortak kolonlarÄ± bul
    common_cols = set(valid_datasets[0].columns)
    for d in valid_datasets[1:]:
        common_cols &= set(d.columns)
    common_cols = list(common_cols)
    
    print(f"Ortak kolon sayÄ±sÄ±: {len(common_cols)}")
    
    df = pd.concat([d[common_cols] for d in valid_datasets], ignore_index=True)
else:
    df = df_sample

print(f"Toplam eÄŸitim verisi: {len(df)} satÄ±r")

# 4. Preprocessing
categorical_cols = ["proto", "service", "state", "attack_cat"]
le_dict = {}

for col in categorical_cols:
    if col in df.columns:
        le = LabelEncoder()
        df[col] = df[col].astype(str)
        # Bilinmeyen deÄŸerler iÃ§in 'unknown' ekle
        unique_vals = list(df[col].unique()) + ['unknown']
        le.fit(unique_vals)
        df[col] = le.transform(df[col])
        le_dict[col] = le

# 5. Model EÄŸitimi (Features vs Label)
# Label sÃ¼tunu: 'label' (0=Normal, 1=Attack)
# EÄŸer label yoksa oluÅŸtur (Sample veride var, user veride var mÄ±?)
if 'label' not in df.columns:
    df['label'] = 0 # VarsayÄ±lan normal

X = df.drop(columns=['label', 'attack_cat', 'id'], errors='ignore')

# Feature isimlerini kaydet (api mismatch olmasÄ±n diye)
feature_names = list(X.columns)
print(f"EÄŸitilen feature sayÄ±sÄ±: {len(feature_names)}")

y = df['label']

print("ğŸ§  Model eÄŸitiliyor...")
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X, y)

# 6. Kaydet
print("ğŸ’¾ Model kaydediliyor...")
with open('ids_model.pkl', 'wb') as f:
    pickle.dump(model, f)

print("ğŸ’¾ Encoderlar kaydediliyor...")
with open('encoders.pkl', 'wb') as f:
    pickle.dump(le_dict, f)

# Feature isimlerini de pickle objesine ekle (sklearn zaten yapar ama biz emin olalÄ±m)
model.feature_names_in_ = np.array(feature_names)

print("âœ… BaÅŸarÄ±lÄ±! ids_model.pkl gÃ¼ncellendi.")

"""
Model KarÅŸÄ±laÅŸtÄ±rma Raporu - LogIz IDS
TÃ¼m modelleri 175K UNSW-NB15 veri setiyle deÄŸerlendirir
"""

import pandas as pd
import numpy as np
import pickle
import io
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score
)
from datetime import datetime

def load_and_clean_csv(filename):
    """CSV dosyasÄ±nÄ± okur ve tÄ±rnak iÅŸaretlerini temizler"""
    with open(filename, 'r', encoding='utf-8') as f:
        content = f.read()
    
    lines = content.splitlines()
    cleaned_lines = []
    for line in lines:
        line = line.strip()
        if line.startswith('"') and line.endswith('"'):
            line = line[1:-1]
        cleaned_lines.append(line)
    
    cleaned_content = "\n".join(cleaned_lines)
    return pd.read_csv(io.StringIO(cleaned_content))

print("=" * 70)
print("ðŸ”¬ MODEL KARÅžILAÅžTIRMA RAPORU - LogIz IDS")
print("=" * 70)
print(f"ðŸ“… Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print()

# Veri setini yÃ¼kle
print("ðŸ“Š Veri seti yÃ¼kleniyor (123.csv - 175K satÄ±r)...")
df = load_and_clean_csv('123.csv')
print(f"   Toplam kayÄ±t: {len(df):,}")

# SaldÄ±rÄ± daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster
if 'attack_cat' in df.columns:
    print("\nðŸ“ˆ SaldÄ±rÄ± TÃ¼rÃ¼ DaÄŸÄ±lÄ±mÄ±:")
    attack_dist = df['attack_cat'].value_counts()
    for attack_type, count in attack_dist.items():
        pct = (count / len(df)) * 100
        print(f"   {attack_type}: {count:,} ({pct:.1f}%)")

# EncoderlarÄ± yÃ¼kle
print("\nðŸ“‚ Encoderlar yÃ¼kleniyor...")
with open('encoders.pkl', 'rb') as f:
    encoders = pickle.load(f)

# Kategorik kolonlarÄ± encode et
categorical_cols = ["proto", "service", "state", "attack_cat"]
df_encoded = df.copy()

for col in categorical_cols:
    if col in df_encoded.columns and col in encoders:
        le = encoders[col]
        df_encoded[col] = df_encoded[col].astype(str)
        known_classes = set(le.classes_)
        fallback_value = 'unknown' if 'unknown' in known_classes else le.classes_[0]
        df_encoded[col] = df_encoded[col].apply(lambda x: x if x in known_classes else fallback_value)
        df_encoded[col] = le.transform(df_encoded[col])

# Feature ve Label ayÄ±r
X = df_encoded.drop(columns=['label', 'attack_cat', 'id'], errors='ignore')
y = df_encoded['label']

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nðŸ“Š Veri BÃ¶lÃ¼mÃ¼:")
print(f"   EÄŸitim seti: {len(X_train):,} kayÄ±t")
print(f"   Test seti: {len(X_test):,} kayÄ±t")
print(f"   Feature sayÄ±sÄ±: {len(X.columns)}")

# Model listesi
models_to_compare = [
    {
        'name': 'Random Forest',
        'file': 'ids_model.pkl',
        'type': 'sklearn'
    },
    {
        'name': 'XGBoost',
        'file': 'xgboost_ids_model.pkl',
        'type': 'xgboost'
    }
]

results = []

print("\n" + "=" * 70)
print("ðŸ§ª MODEL DEÄžERLENDÄ°RMELERÄ°")
print("=" * 70)

for model_info in models_to_compare:
    print(f"\n{'â”€' * 50}")
    print(f"ðŸ”® {model_info['name']} Modeli")
    print(f"{'â”€' * 50}")
    
    try:
        # Model yÃ¼kle
        with open(model_info['file'], 'rb') as f:
            model = pickle.load(f)
        
        # Feature hizalama
        X_test_aligned = X_test.copy()
        if hasattr(model, 'feature_names_in_'):
            expected_cols = list(model.feature_names_in_)
            for col in expected_cols:
                if col not in X_test_aligned.columns:
                    X_test_aligned[col] = 0
            X_test_aligned = X_test_aligned[expected_cols]
        
        # Tahmin zamanÄ± Ã¶lÃ§
        start_time = time.time()
        y_pred = model.predict(X_test_aligned)
        prediction_time = time.time() - start_time
        
        # OlasÄ±lÄ±klar (eÄŸer varsa)
        try:
            y_proba = model.predict_proba(X_test_aligned)[:, 1]
            auc_score = roc_auc_score(y_test, y_proba)
        except:
            auc_score = None
        
        # Metrikler
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        
        # Confusion Matrix
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # SonuÃ§larÄ± kaydet
        result = {
            'Model': model_info['name'],
            'Accuracy': accuracy,
            'Precision': precision,
            'Recall': recall,
            'F1-Score': f1,
            'AUC-ROC': auc_score,
            'True Positive': tp,
            'True Negative': tn,
            'False Positive': fp,
            'False Negative': fn,
            'Prediction Time (s)': prediction_time
        }
        results.append(result)
        
        # SonuÃ§larÄ± yazdÄ±r
        print(f"   ðŸŽ¯ Accuracy:  {accuracy * 100:.2f}%")
        print(f"   ðŸ“ Precision: {precision * 100:.2f}%")
        print(f"   ðŸ” Recall:    {recall * 100:.2f}%")
        print(f"   âš–ï¸  F1-Score:  {f1 * 100:.2f}%")
        if auc_score:
            print(f"   ðŸ“ˆ AUC-ROC:   {auc_score:.4f}")
        print(f"   â±ï¸  SÃ¼re:      {prediction_time:.2f}s ({len(X_test)/prediction_time:.0f} kayÄ±t/sn)")
        print()
        print(f"   Confusion Matrix:")
        print(f"     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        print(f"     â”‚ TN: {tn:>10,} â”‚ FP: {fp:>10,} â”‚")
        print(f"     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
        print(f"     â”‚ FN: {fn:>10,} â”‚ TP: {tp:>10,} â”‚")
        print(f"     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        
    except Exception as e:
        print(f"   âŒ Hata: {e}")
        results.append({
            'Model': model_info['name'],
            'Accuracy': 0,
            'Error': str(e)
        })

# KarÅŸÄ±laÅŸtÄ±rma Ã–zeti
print("\n" + "=" * 70)
print("ðŸ“Š KARÅžILAÅžTIRMA Ã–ZETÄ°")
print("=" * 70)

if len(results) > 1:
    # En iyi modeli bul
    best_accuracy = max(r['Accuracy'] for r in results if 'Accuracy' in r)
    best_f1 = max(r['F1-Score'] for r in results if 'F1-Score' in r)
    
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Model              â”‚ Accuracy  â”‚ Precision â”‚ Recall    â”‚ F1-Score  â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    for r in results:
        if 'Accuracy' in r and r['Accuracy'] > 0:
            acc_str = f"{r['Accuracy']*100:.2f}%"
            prec_str = f"{r['Precision']*100:.2f}%"
            rec_str = f"{r['Recall']*100:.2f}%"
            f1_str = f"{r['F1-Score']*100:.2f}%"
            
            # En iyi deÄŸerleri iÅŸaretle
            if r['Accuracy'] == best_accuracy:
                acc_str = f"â˜…{acc_str}"
            if r['F1-Score'] == best_f1:
                f1_str = f"â˜…{f1_str}"
            
            print(f"â”‚ {r['Model']:<18} â”‚ {acc_str:>9} â”‚ {prec_str:>9} â”‚ {rec_str:>9} â”‚ {f1_str:>9} â”‚")
    
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print("(â˜… = En iyi deÄŸer)")

# Ã–neriler
print("\n" + "=" * 70)
print("ðŸ’¡ Ã–NERÄ°LER")
print("=" * 70)

best_model = max(results, key=lambda x: x.get('F1-Score', 0))
print(f"\nâœ… Ã–nerilen Model: {best_model['Model']}")
print(f"   F1-Score: {best_model.get('F1-Score', 0)*100:.2f}%")

if best_model.get('False Negative', 0) > 0:
    fn_rate = best_model['False Negative'] / (best_model['False Negative'] + best_model['True Positive']) * 100
    print(f"\nâš ï¸  Dikkat: {fn_rate:.2f}% saldÄ±rÄ± kaÃ§Ä±rÄ±lÄ±yor (False Negative)")
    print("   Recall'Ä± artÄ±rmak iÃ§in threshold dÃ¼ÅŸÃ¼rÃ¼lebilir.")

print("\n" + "=" * 70)
print("âœ… Rapor TamamlandÄ±!")
print("=" * 70)

# SonuÃ§larÄ± dosyaya kaydet
report_file = "model_comparison_report.txt"
with open(report_file, 'w', encoding='utf-8') as f:
    f.write("MODEL KARSILASTIRMA RAPORU - LogIz IDS\n")
    f.write("=" * 50 + "\n\n")
    
    for r in results:
        if 'Accuracy' in r and r['Accuracy'] > 0:
            f.write(f"Model: {r['Model']}\n")
            f.write(f"  Accuracy:  {r['Accuracy']*100:.2f}%\n")
            f.write(f"  Precision: {r['Precision']*100:.2f}%\n")
            f.write(f"  Recall:    {r['Recall']*100:.2f}%\n")
            f.write(f"  F1-Score:  {r['F1-Score']*100:.2f}%\n")
            if r.get('AUC-ROC'):
                f.write(f"  AUC-ROC:   {r['AUC-ROC']:.4f}\n")
            f.write(f"  TP: {r['True Positive']}, TN: {r['True Negative']}\n")
            f.write(f"  FP: {r['False Positive']}, FN: {r['False Negative']}\n")
            f.write(f"  Prediction Time: {r['Prediction Time (s)']:.2f}s\n")
            f.write("\n")
    
    f.write(f"\nOnerilen Model: {best_model['Model']}\n")

print(f"\nðŸ“„ Rapor kaydedildi: {report_file}")
